%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%  devel.tex
%
%  Information for users and delvelopers
%
%  Edinburgh Soft Matter and Statistical Physics Group and
%  Edinburgh Parallel Computing Centre
%
%  (c) 2014 The University of Edinburgh
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{General Information for Users and Developers}

This section contains information on the design of the code, along
with details of compilation an testing procedures
which may be of interest to both users and developers.
\textit{Ludwig} is named for Ludwig Boltzmann (1844--1906) to
reflect its background in lattice Boltzmann hydrodynamics.

\subsection{Manifest}

The top level ludwig directory should contain at least the following:
\begin{lstlisting}
$ ls
bash-3.2$ ls
config  LICENSE      mpi_s   src       tests  version.h
docs    Makefile.mk  README  targetDP  util
\end{lstlisting}
The code is released under a standard BSD license; the current version
number is found in \texttt{version.h}. The main source is found in
the \texttt{src} directory, with other relevant code in \texttt{tests}
and \texttt{util}. The \texttt{targetDP} directory holds code related
to the targetDP abstraction layer \cite{gray2013}. These documents are
found in \texttt{docs}.

\subsection{Code Overview}

The code is ANSI C (1999), and can be built without any external
dependencies, with the exception of the message passing interface
(MPI), which is used to provide domain-decomposition based parallelism.
Note that the code will also compile under NVIDIA \texttt{nvcc}, i.e.,
it also meets the C++ standard.

\subsubsection{Design}

The \textit{Ludwig} code has evolved and expanded over a number of
years to its present state. Its original purpose was to investigate
specifically spinodal decomposition in binary fluids (see, e.g.,
\cite{kendon2001}). This work used a free-energy based formulation
combined with a two-distribution approach to the binary fluid problem
(with lattice Boltzmann model D3Q15 at that time). This approach  is
retained today, albeit in a somewhat updated form. The wetting problem
for binary fluid at solid surfaces has also been of consistent interest.
The code has always been developed with
parallel computing in mind, and has been run on a large number of
different parallel machines including the Cray T3E at Edinburgh. These
features, developed by J.-C. Desplat and others,  were reflected in the
early descriptive publication in \textit{Comp.\ Phys.\ Comm} in 2001
\cite{desplat2001}. 

The expansion of the code to include a number of additional features
has occasioned significant alterations over time, and little of the
original code remains. However, the fundamental idea that the code
should essentially operate for high performance computers and be
implemented using ANSI C with message passing via MPI has remained
unchanged.

The code has a number of basic building blocks which are encapsulated
in individual files.

\subsubsection{Parallel environment}

The code is designed around message passing using MPI. A stub MPI
library is provided for platforms where a real MPI is not available
(an increasingly rare occurrence), in which case the code runs in
serial. The parallel environment (interface defined in \texttt{pe.h})
therefore underpins
the entire code and provides basic MPI infrastructure such as
\texttt{info()},
which provides \texttt{printf()} functionality at the root process
only. The parallel environment also provides information on version
number etc. MPI parallelism is implemented via standard domain
decomposition based on the computational lattice (discussed
further in the following section on the coordinate system).

\subsubsection{targetDP: threaded parallelism and vector parallelism}

To allow various threaded models to be included without duplicating
source code, we have developed a lightweight abstraction of the
thread level parallelism, This currently supports a standard single-threaded
model, OpenMP, or CUDA. targetDP (``target data parallel'') allows single
kernels to be written which can then be compiled for the different threaded
models which may be appropriate on different platforms. targetDP can also
be used as a convenient way to express vector parallelism
(typically SSE or AVX depending on processor architecture). targetDP allows
explicit specification of the vector length at compile time.

targetDP does not automatically identify parallelism; this must still be
added appropriately by the developer. It is merely a concise way to include
different threaded models. It is only available in the development version.

\subsubsection{Coordinate system}

It is important to understand the coordinate system used in the
computation. This is fundamentally a regular, 3-dimensional Cartesian
coordinate system . (Even if a D2Q9 LB model is employed, the code is
still fundamentally 3-dimensional, so it is perhaps not as efficient  
for 2-dimensional problems as it might be.)

The coordinate system is centred around the (LB) lattice, with lattice
spacings $\Delta x = \Delta y = \Delta z = 1$. We will refer, in general,
to the lattice spacing as $\Delta x$ throughout, its generalisation to
three dimensions $x,y,z$ being understood. Lattice sites in the
$x-$direction therefore have unit spacing and are located at
$x = 1, 2, \ldots, N_x$.
The length of the system $L_x = N_x$, with the limits of
the computational domain begin $x = 1/2$ and $x = L_x + 1/2$. This allows
us to specify a unit control volume centred on each lattice site $x_i$
being $i-1/2$ to $i + 1/2$. This will become particularly significant for
finite difference (finite volume) approaches discussed later.

Information on the coordinate system, system size and so on is
encapsulated in \texttt{coords.c}, which also deals with the
regular domain decomposition in parallel. Decompositions may be
explicitly requested by the user in the input. or computed by
the code itself at run time. \texttt{coords.h} also provides
basic functionality for message passing within a standard MPI
Cartesian communicator, specification of periodic boundary
conditions, and so on.

Three-dimensional fields are typically stored on the lattice,
but are addressed in compressed one-dimensional format. This
avoids use of multidimensional arrays in C. This addressing
must take account of the width of the halo region required at
the edge of each sub-domain required for exchanging information
in the domain decomposition. The extent of the halo region
varies depending on the application required, and is selected
automatically at run time.

\subsubsection{Lattice Boltzmann hydrodynamics}

The hydrodynamic core of the calculation is supplied by the lattice
Boltzmann method, which was central at the time of first development.
LB is also the basis for hydrodynamic solid-fluid interactions at
stationary walls and for moving spherical colloids. The LB approach
is described in more detail in Section~\ref{section:lb-hydrodynamics}.
For general
and historical references, the interested reader should consider,
e.g., Succi \cite{succi}.

\subsubsection{Free energy}

For complex fluids, hydrodynamics is augmented by the addition of
a free energy for the problem at hand, expressed in terms of an
appropriate order parameter. The order parameter may be a three
dimensional field, vector field, or tensor field. For each problem
type, appropriate tome evolution of the order parameter is supplied.

Coupling between the thermodynamic sector and the hydrodynamics is
abstracted so that the hydrodynamic core does not require alteration
for the different free energies. This is implemented via a series of
call back functions in the free energy which are set appropriately
at run time to correspond to that specified by the user in the
input.

Different (bulk) fluid free energy choices are complemented by
appropriate surface free energy contributions which typically
alter the computation of order parameter gradients at solid
surface. Specific gradient routines for the calculation of
order parameter gradients may be selected or added by the
user or developer.

Currently available free energies are:
\begin{itemize}
\item Symmetric binary fluid with scalar compositional order parameter
$\phi(\mathbf{r})$ and related Cahn-Hilliard equation;
\item Brazovskii smectics, again which scalar compositional order parameter
$\phi(\mathbf{r})$;
\item Polar (active) gels with vector order parameter $P_\alpha (\mathbf{r})$
and related Leslie-Erikson equation;
\item Landau-de Gennes liquid crystal free energy with tensor
orientational order parameter $Q_{\alpha\beta}(\mathbf{r})$, extended to
apolar active fluids and related Beris-Edwards equation;
\item a free energy appropriate for electrokinetics for charged fluids
and related Nernst-Planck equations (also requiring the solution of the
Poisson equation for the potential);
\item a coupled electrokinetic binary fluid model;
\item a liquid crystal emulsion free energy which couples a binary
composition to the liquid crystal.
\end{itemize}
See the relevant sections on each free energy for further details.

\subsection{Compilation}

Compilation of the main code is controlled by the \texttt{config.mk} in
the  top-level directory. A number of example \texttt{config.mk} files
are provided in the \texttt{config} directory (which can be copied and
adjusted as needed). This section discusses a number of issues
which influence compilation.

\subsubsection{Dependencies on third-party software:}
There is the option to use PETSC to solve the Poisson equation required in
the electrokinetic problem. A rather less efficient in-built method
can be used if PETSC is not available.
We suggest using PETSC
v3.4 or later available from Argonne National Laboratory
\texttt{http://www.mcs.anl.gov/petsc/}.

The tests use the lightweight implementation of exceptions provided
under the GPL Lesser General Public License by Guillermo Calvo. This
is included with the source.

\subsubsection{Makefile}

The \texttt{Makefile.mk} and \texttt{config.mk} files in the top level
directory control options for compilation. Before compilation,
the \texttt{config.mk} file must be edited provide details of the local
C compiler(s). The relevant lines are usually limited to, e.g.:
\begin{lstlisting}
CC = cc
MPICC = mpicc
CFLAGS = -O2
\end{lstlisting}
where \texttt{CC} is the compiler used for serial compilation,
\texttt{MPICC} is the compiler used for compilation against
MPI, and \texttt{CFLAGS} provides compiler switches (often
related to optimisation).

The \texttt{Makefile} provided in each subdirectory includes
the \texttt{config.mk} file via the top level \texttt{Makefile.mk}.
Changes to the individual Makefiles should therefore not be required.

If GPU compilation is required, the \texttt{config.mk} file should
specify the local details concerning \texttt{nvcc}. This includes
explicit information on the location of MPI headers and libraries
if an MPI-parallel GPU version is required.

\subsubsection{C assertions}

The code makes quite a lot of use of standard C assertions, which
are useful to prevent errors. They do result in a considerably
slower execution in some instances, so production runs should
switch off the assertions with the standard \texttt{NDEBUG}
preprocessor flag. Add \texttt{-DNDEBUG} to \texttt{CFLAGS} in
the \texttt{config.mk}.

\subsubsection{Targets for serial and parallel code}

The code can be compiled with or without a true MPI library.
For serial execution, the MPI stub library must be compiled
first (see ``Quick Start for Users'' Section \ref{section:quick-serial}).
The appropriate target is:
\begin{lstlisting}
bash-3.2$ make serial
make serial-d3q19
make serial-model ``LB=-D_D3Q19_'' ``LBOBJ=d3q19.o''
...
\end{lstlisting}
from which it will be seen that the default LB model is D3Q19. This is
the same for the true MPI target
\begin{lstlisting}
bash-3.2$ make mpi
make mpi-d3q19
make mpi-model ``LB=-D_D3Q19_'' ``LBOBJ=d3q19.o''
...
\end{lstlisting}

\subsubsection{Targets for different LB models}

Specific targets are provided if an alternative LB model is wanted.
The available options are:
\begin{lstlisting}
make [ serial-d2q9 | serial-d3q15 | serial-d3q19 ]
make [ mpi-d2q9 | mpi-d3q15 | mpi-d3q19 ]
\end{lstlisting}
A further set of targets is supplied if `reverse' or structure-of-array
memory ordering is wanted (e.g., for GPU computing):
\begin{lstlisting}
make [ serial-d2q9r | serial-d3q15r | serial-d3q19r ]
make [ mpi-d2q9r | mpi-d3q15r | mpi-d3q19r ]
\end{lstlisting}
This will only influence efficiency of the code: results are unchanged.

\subsection{Tests}

A series of tests are provided in the \texttt{./tests} directory. These
are of two types. Unit tests are generally written when units of code
are first introduced to ensure basic operation is error free. The unit
tests are encoded in an executable linked against the appropriate
\textit{Ludwig} library.
Regression
tests are introduced and updated when physical results are (broadly)
validated. The regression tests use the stand-alone executable for
the appropriate model, and read input files which define the different
tests.
Both are run regularly as the basis of a nightly test procedure, but
they can be run independently, e.g., after adding or changing code.


\subsubsection{Running the tests}

A number of different \texttt{Makefile} targets are provided for
running serial, parallel,
or GPU tests. For each test, there is the option to run either
the relevant unit tests, or regression tests, or both. For example, to
run both serial unit tests and serial regression tests for the D3Q19
model, invoke
\begin{lstlisting}
$ cd tests
$ make compile-run-serial
\end{lstlisting}
in the test directory.
The model selection follows the naming scheme described for compilation
in the previous section.


\subsection{Additional Notes for Developers}
\label{xref:developers}

Developers who wish to contribute code to the SVN repository
please consider the following pleas concerning standards.

\subsubsection{Coding standards}

While definitive statements on style are avoided, please try to
maintain some basic standards: (0) code should be strictly ANSI C99
standard; (1) code should compile without warnings when appropriate
compiler flags are set (e.g., \texttt{-Wall} under \texttt{gcc});
(2) avoid long lines of code for readability reasons;
(3) avoid confusing commented-out code; (4) avoid conditional pre-processor
directives if possible; (5) add meaningful and descriptive comments;
(6) use standard \texttt{assert()} to trap programming errors;
(7) explicitly trap possible run time errors (8) add appropriate
tests.

\subsubsection{Documentation standards}

Additions and alterations to the code need to be reflected in the
documentation. These should be checked in at the same time as the
code itself.

\subsubsection{Protocol}

Before checking in code, please follow procedure: (1) increment the
patch version number in \texttt{version.h} consistent with the SVN
(if a minor version increment is required, communication with
other developers must be considered); (2) run at least the unit
tests and the short regression tests; (3) add a note to the change
log; and (4) SVN update and commit.

\vfill
\pagebreak

